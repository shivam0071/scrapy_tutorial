1. ) First create a virtual env
conda create --name scrappyenv

2.) activate scrappyenv

3.) conda --install -c conda-forge scrapy

4.) CREATING a scrapy project
        scrapy startproject basics1

5.) starting a spider or script
    scrapy crawl spider_name

5.1 ) Saving output in a JSON file
    scrapy crawl quotes -o quotes.json [list of dictionary | list of dictionary]

5.2) Saving data in json line JL format
    scrapy crawl quotes -o quotes.jl [dictionary | dict | dict ]

6.) Extracting data
    scrapy shell "http://quotes.toscrape.com/page/1/"
    response.css('title')
    [<Selector xpath='descendant-or-self::title' data='<title>Quotes to Scrape</title>'>]

    response.css('title::text').getall()
    ['Quotes to Scrape']

     response.css('title').getall()
    ['<title>Quotes to Scrape</title>']

    response.css('title::text').get()
    'Quotes to Scrape'

    response.css('title::text')[0].get()
    'Quotes to Scrape


for most scraping code, you want it to be resilient to errors due to things
not being found on a page, so that even if some parts fail to be scraped,
you can at least get some data

    RE

    response.css('title::text').re(r'Quotes.*')
    ['Quotes to Scrape']
    >>> response.css('title::text').re(r'Q\w+')
    ['Quotes']
    >>> response.css('title::text').re(r'(\w+) to (\w+)')
    ['Quotes', 'Scrape']

    XPATH
    >>> response.xpath('//title')
    [<Selector xpath='//title' data='<title>Quotes to Scrape</title>'>]
    >>> response.xpath('//title/text()').get()
    'Quotes to Scrape'

    To get ANY ATTRIBUTE of a tag
    <ul class="pager">
        <li class="next">
            <a href="/page/2/">Next <span aria-hidden="true">&rarr;</span></a>
        </li>
    </ul>

    response.css('li.next a').get()
    '<a href="/page/2/">Next <span aria-hidden="true">â†’</span></a>'

    response.css('li.next a::attr(href)').get()
    '/page/2/'

    OR
    response.css('li.next a').attrib['href']
    '/page/2'